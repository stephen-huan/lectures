\documentclass{beamer}                             % presentation
% \documentclass[draft]{beamer}                    % improves compile time
% 4' x 3' poster (48" x 36")
\usepackage[orientation=landscape,size=custom,width=121.92,height=91.44,
scale=1.4,debug]{beamerposter}
% replace default beamer blocks for more customization of outlines
% https://tex.stackexchange.com/questions/11484/how-to-draw-a-frame-box-around-an-arbitrary-large-piece-of-text-figures-whatever
\usepackage{tcolorbox}
\usepackage[utf8]{inputenc}                        % utf8
\usepackage[T1]{fontenc}                           % fix font encoding
\usepackage{lmodern}                               % arbitrarily large font
\usepackage[english]{babel}                        % language
\usepackage{geometry, hyperref, fancyhdr, algorithm}
\usepackage{amsmath, amssymb, amsthm}              % ams mathematical packages
\usepackage{physics, mathtools, bm}                % extra math packages
\usepackage{graphicx, subcaption, wrapfig}         % images
\usepackage{svg}                                   % svg images
\usepackage{fvextra, textcomp, CJKutf8}            % misc. text formatting
\usepackage[autostyle, english=american]{csquotes} % quotes
\usepackage{tikz, pgfplots}                        % plots and graphs
\usepackage[noend]{algpseudocode}                  % algorithm psuedocode
\usepackage[cache=true]{minted}                    % source code
\usepackage[style=ieee]{biblatex}                  % bibliography

\usetikzlibrary{positioning}                       % advanced positioning
\pgfplotsset{compat=newest}                        % version of pgfplots
\usepgfplotslibrary{fillbetween}

\graphicspath{{./figures/}}
\addbibresource{cholesky.bib}

\newcommand{\blocktitle}[1]{{\Large \textbf{#1}}}

% serif font in math mode
% https://ctan.math.utah.edu/ctan/tex-archive/fonts/lm/tex/latex/lm/lmodern.sty
% \usefonttheme[onlymath]{serif}
\renewcommand\mathfamilydefault{cmr}

\SetSymbolFont{operators}   {normal}{OT1}{cmr} {m}{n}
\SetSymbolFont{letters}     {normal}{OML}{cmm} {m}{it}
\SetSymbolFont{symbols}     {normal}{OMS}{cmsy}{m}{n}
\SetSymbolFont{largesymbols}{normal}{OMX}{cmex}{m}{n}
\SetSymbolFont{operators}   {bold}  {OT1}{cmr} {bx}{n}
\SetSymbolFont{letters}     {bold}  {OML}{cmm} {b}{it}
\SetSymbolFont{symbols}     {bold}  {OMS}{cmsy}{b}{n}
\SetSymbolFont{largesymbols}{bold}  {OMX}{cmex}{m}{n}

\SetMathAlphabet{\mathbf}{normal}{OT1}{cmr}{bx}{n}
\SetMathAlphabet{\mathsf}{normal}{OT1}{cmss}{m}{n}
\SetMathAlphabet{\mathit}{normal}{OT1}{cmr}{m}{it}
\SetMathAlphabet{\mathtt}{normal}{OT1}{cmtt}{m}{n}
\SetMathAlphabet{\mathbf}{bold}  {OT1}{cmr}{bx}{n}
\SetMathAlphabet{\mathsf}{bold}  {OT1}{cmss}{bx}{n}
\SetMathAlphabet{\mathit}{bold}  {OT1}{cmr}{bx}{it}
\SetMathAlphabet{\mathtt}{bold}  {OT1}{cmtt}{m}{n}

\renewcommand{\vec}[1]{\bm{#1}}
\newcommand*{\Id}{\mathrm{Id}}
\newcommand*{\defeq}{\coloneqq}
\newcommand*{\BigO}{\mathcal{O}}
\newcommand*{\SpSet}{\mathcal{S}}
\newcommand*{\N}{\mathcal{N}}
\renewcommand*{\I}{I}

\DeclareMathOperator{\logdet}{logdet}
\DeclareMathOperator{\chol}{chol}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\newcommand*{\CM}{\Theta}
\newcommand*{\Train}{\mathrm{Tr}}
\newcommand*{\Pred}{\mathrm{Pr}}

\DeclarePairedDelimiterX{\infdivx}[2]{(}{)}{%
  #1\;\delimsize\|\;#2%
}
\newcommand*{\KL}{\mathbb{D}_{\operatorname{KL}}\infdivx}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\Var}{\mathbb{V}ar}
\DeclareMathOperator{\Cov}{\mathbb{C}ov}
\DeclareMathOperator{\Corr}{\mathbb{C}orr}
\DeclareMathOperator{\entropy}{\mathbb{H}}
\DeclareMathOperator{\MI}{\mathbb{I}}

%%% colors

\definecolor{lightblue}{HTML}{a1b4c7}
\definecolor{orange}{HTML}{ea8810}
\definecolor{silver}{HTML}{b0aba8}
\definecolor{rust}{HTML}{b8420f}
\definecolor{seagreen}{HTML}{23553c}

\colorlet{lightsilver}{silver!20!white}
\colorlet{darkorange}{orange!85!black}
\colorlet{darksilver}{silver!85!black}
\colorlet{darklightblue}{lightblue!75!black}
\colorlet{darkrust}{rust!85!black}
\colorlet{darkseagreen}{seagreen!85!black}

\colorlet{zeroborder}{darksilver}
\colorlet{zerocolor}{lightsilver}
\colorlet{nnzborder}{darksilver}
\colorlet{nnzcolor}{silver}

\colorlet{colborder}{black}
\colorlet{targetcolor}{orange}
\colorlet{selcolor}{seagreen}
\colorlet{candcolor}{lightblue}

\hypersetup{
  colorlinks=true,
  linkcolor=darkrust,
  citecolor=darkseagreen,
  urlcolor=darksilver
}

\pgfplotsset{
  every axis plot/.append style={line width=4},
  % doesn't seem to do anything, debug later
  every mark/.append style={mark size=32},
}

%%% beamer settings

\usetheme{Pittsburgh}

% hide navigation buttons
\setbeamertemplate{navigation symbols}{}
% change title color
\setbeamercolor{title}{fg=darklightblue}
\setbeamercolor{frametitle}{fg=darklightblue}
% change bibliography entry colors
\setbeamercolor{bibliography entry author}{fg=darklightblue}
\setbeamercolor{bibliography entry note}{fg=lightblue}

% blocks - use tcolorbox instead of beamer blocks
% \setbeamercolor{block title}{fg=black}
% \setbeamercolor{block body}{bg=lightblue}

% https://mirrors.concertpass.com/tex-archive/macros/latex/contrib/tcolorbox/tcolorbox.pdf
% \tcbuselibrary{skins}
\tcbset{
  % skin=enhanced,
  colframe=darklightblue,
  colback=white,
  % coltitle=black,
  % colbacktitle=white,
  % fonttitle={\Large\bfseries},
  % titlerule=0mm,
  % titlerule style={draw=none, line width=0mm},
}

% margin
\setbeamersize{text margin left=1.27cm, text margin right=1.27cm}

% title page
\title[]{\Huge Sparse Cholesky Factorization by \\
Greedy Conditional Selection}
\subtitle{}
\author[Huan]{\LARGE Stephen Huan and Florian Sch{\"a}fer}
% \institute[Georgia Institute of Technology]
% {
%   Georgia Institute of Technology
% }
\date[]{}
\subject{Computer Science}

\begin{document}
% top align frame
% https://tex.stackexchange.com/questions/16447/beamer-top-aligning-columns-within-a-top-aligned-frame
\begin{frame}[t]
\titlepage

% GT logo in top left
\begin{tikzpicture}[overlay,remember picture]
  \node[below right=4cm and 1.27cm] at (current page.north west) {
      \includesvg[width=0.25\columnwidth]{GTLogoSeal_RGB}
  };
\end{tikzpicture}

% QR code in top right
\begin{tikzpicture}[overlay,remember picture]
  \node[below left=1.27cm and 1.27cm] at (current page.north east) {
      \includesvg[width=0.1\columnwidth]{qrcode}
    };
\end{tikzpicture}

% remove gap
\vspace{-8cm}

\begin{center}
  \textcolor{darksilver}{\rule{\textwidth}{2mm}}
\end{center}

\begin{columns}[T]

%%% column 1

\begin{column}{0.30\textwidth}
  \begin{tcolorbox}
    \blocktitle{The problem: Gaussian process regression}

    Given measurements \( \vec{y}_\Train \) at \( N \) points \(
    X_\Train \), we wish to estimate unseen data \( \vec{y}_\Pred
    \) at \( X_\Pred \). Estimation of \( \vec{y}_\Pred \) can
    be done by conditioning on \( \vec{y}_\Train \):
    \begin{align*}
      \E[\vec{y}_\Pred \mid \vec{y}_\Train] &=
        \vec{\mu}_\Pred +
        \CM_{\Pred, \Train} \CM_{\Train, \Train}^{-1}
        (\vec{y}_\Train - \vec{\mu}_\Train) \\
      \Cov[\vec{y}_\Pred \mid \vec{y}_\Train] &=
        \CM_{\Pred, \Pred} -
        \CM_{\Pred, \Train} \CM_{\Train, \Train}^{-1}
        \CM_{\Train, \Pred}
      \defeq \CM_{\Pred, \Pred \mid \Train}
    \end{align*}
  \end{tcolorbox}

  \begin{tcolorbox}
    \blocktitle{Cubic bottleneck and the screening effect}

    Computing the conditional distribution has computational
    cost \( \BigO(N^3) \), which is infeasible for many points.
    Instead, exploit the \emph{screening effect}: conditional
    on nearby points, far away points have little correlation.
    \begin{figure}[t]
      \centering
      \input{figures/screening/uncond.tex}%
      \input{figures/screening/cond.tex}
      \label{fig:screening}
    \end{figure}
  \end{tcolorbox}

  \begin{tcolorbox}
    \blocktitle{\textit{k}-nearest neighbors?}
    \vspace{0.25\baselineskip}

    \begin{columns}
      \begin{column}{0.5\textwidth}
        The screening effect suggests that one should simply
        pick the \( k \) closest points, recovering the
        \( k \)-nearest neighbors (\( k \)-NN) algorithm.

        \vspace{\baselineskip}
        Here, the \textcolor{lightblue}{blue} points are the
        \textcolor{lightblue}{candidates}, the \textcolor{orange}{orange}
        point is the \textcolor{orange}{unknown} point, and the
        \textcolor{darkseagreen}{green} points are the \( k \)
        \textcolor{darkseagreen}{selected} points

        (in this example, \( k = 2 \)).
      \end{column}
      \begin{column}{0.5\textwidth}
        \begin{figure}[t]
          \centering
          \input{figures/selection/knn.tex}
          \label{fig:selection_knn}
        \end{figure}
      \end{column}
    \end{columns}
  \end{tcolorbox}

  \begin{tcolorbox}
    \blocktitle{\textit{k}-NN is myopic, account for conditioning!}
    \textbf{
      Algorithm [conditional \textit{k}-nearest neighbors (C\textit{k}-NN)]:
    }
    \vspace{-0.5\baselineskip}
    \begin{columns}
      \begin{column}{0.5\textwidth}
        Selecting the closest point every iteration leads to redundancy.

        \vspace{\baselineskip}
        Instead, select points \emph{conditional} on points already selected.
        Selecting points by \emph{information} instead of by distance
        motivates conditional \( k \)-th nearest neighbors (C\( k \)-NN).
      \end{column}
      \begin{column}{0.5\textwidth}
        \begin{figure}[t]
          \centering
          \input{figures/selection/cknn.tex}
          \label{fig:selection_cknn}
        \end{figure}
      \end{column}
    \end{columns}
  \end{tcolorbox}
\end{column}

% https://tex.stackexchange.com/questions/504412/how-to-put-a-vertical-rule-between-beamer-columns-created-with-the-column-comma
% conveniently the rule doubles as a strut
\begin{column}{0.01\textwidth}
  \begin{center}
    \textcolor{darksilver}{\rule[-1cm]{1mm}{0.8\textheight}}
  \end{center}
\end{column}

%%% column 2

\begin{column}{0.30\textwidth}
  \begin{tcolorbox}
    \blocktitle{Greedy mutual information maximization}

    Greedily select the next training point with highest mutual
    information with the target point. If \( \I \) is the set
    of selected indices, select by:
    \begin{align*}
      \argmax_{j \not \in \I} \: \Corr[y_\Pred, y_j \mid \I]^2
    \end{align*}
  \end{tcolorbox}

  \begin{tcolorbox}
    \blocktitle{Efficient computation from Cholesky factor}

    A direct computation of the objective takes \(
    \BigO(N k^4) \) to select \( k \) points.

    This computational cost can be reduced to \( \BigO(N
    k^2) \) by storing a partial Cholesky factor, since
    each column is conditional on everything before it:
    \begin{align*}
      \chol(\CM) &=
      \begin{pmatrix}
        \Id & 0 \\
        \textcolor{darkorange}{\CM_{2, 1} \CM_{1, 1}^{-1}} & \Id
      \end{pmatrix}
      \begin{pmatrix}
        \chol(\CM_{1, 1}) & 0 \\
        0 & \chol(\textcolor{lightblue}{
          \CM_{2, 2} - \CM_{2, 1} \CM_{1, 1}^{-1} \CM_{1, 2}
        })
      \end{pmatrix}
    \end{align*}
  \end{tcolorbox}

  \begin{tcolorbox}
    \blocktitle{Generalization to multiple prediction points}

    For multiple prediction points, the objective becomes to minimize the
    log determinant of the conditional covariance matrix of prediction points.
    By making use of the matrix determinant lemma, one can show that:
    \begin{align*}
      \logdet(\CM_{\Pred, \Pred \mid \I, k})
        - \logdet(\CM_{\Pred, \Pred \mid \I})
      &= \log(\CM_{k, k \mid \I, \Pred})
        - \log(\CM_{k, k \mid \I})
    \end{align*}
    We can efficiently compute the objective by storing \emph{two}
    Cholesky factors, yielding a complexity of \( \BigO(N
    k^2 + N m^2 + m^3) \) for \( m \) prediction points.
  \end{tcolorbox}

  \begin{tcolorbox}
    \blocktitle{Global approximation by KL-minimization}

    Approximate a Gaussian process by a sparse approximate Cholesky factor
    of its precision. Measure the resulting approximation accuracy by
    the KL divergence between the corresponding centered Gaussian processes:
    \begin{align*}
      L \coloneqq \argmin_{\hat{L} \in \SpSet} \,
        \KL*{\N(\vec{0}, \CM)}
            {\N(\vec{0}, (\hat{L} \hat{L}^{\top})^{-1})}
    \end{align*}

    Using the optimal unique minimizer \( L \) from closed-form computation:
    \begin{align*}
      L_{s_i, i} = \frac{\CM_{s_i, s_i}^{-1} \vec{e}_1}
      {\sqrt{\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1}}
    \end{align*}

    Objective becomes minimize variance of \(
    i \)th point, conditional on selected!
    \begin{align*}
      \KL*{\CM}{(L L^{\top})^{-1}} &\propto
      \sum_{i = 1}^N
        \left [
          \log \left ( \CM_{i, i \mid s_i \setminus \{ i \}} \right ) -
          \log \left ( \CM_{i, i \mid i + 1:} \right )
        \right ]
    \end{align*}
    % Sparsity of column of Cholesky factor \( \iff \) sparsity in selection
  \end{tcolorbox}

  \begin{tcolorbox}
    \blocktitle{Applying selection to Cholesky factorization}
    \vspace{-0.5\baselineskip}

    \begin{columns}
      \begin{column}{0.5\textwidth}
        For a column in isolation, \textcolor{orange}{unknown}
        point is the \textcolor{orange}{diagonal entry},
        \textcolor{lightblue}{below it} are \textcolor{lightblue}{candidates},
        and add \textcolor{seagreen}{selected} entries to the
        \textcolor{seagreen}{sparsity pattern} \( s_i \).

        \vspace{\baselineskip}
        However, for aggregated columns (supernodes), a
        candidate can be added between prediction points.

        By careful application of rank-one downdating, this
        structure can be preserved at no additional cost.
      \end{column}
      \begin{column}{0.5\textwidth}
        \vspace{-\baselineskip}
        \begin{figure}[t]
          \centering
          \begin{tikzpicture}[baseline,scale=1]
            \input{figures/cholesky_factor.tex}
            \node[] at (12, -3) {
              \input{figures/selection/cknn2.tex}
            };
          \end{tikzpicture}
        \end{figure}
      \end{column}
    \end{columns}
  \end{tcolorbox}
\end{column}

\begin{column}{0.01\textwidth}
  \begin{center}
    \textcolor{darksilver}{\rule[-1cm]{1mm}{0.8\textheight}}
  \end{center}
\end{column}

%%% column 3

\begin{column}{0.30\textwidth}
  \begin{tcolorbox}
    \blocktitle{Drop-in replacement for \textit{k}-NN on MNIST}

    \begin{columns}
      \begin{column}{0.5\textwidth}
        We classify an image by taking the mode label in
        \( k \) selected images. C\( k \)-NN gives better
        accuracies on the MNIST dataset for every \( k > 2 \).
      \end{column}
      \begin{column}{0.5\textwidth}
        \pgfplotsset{
          cycle list={
            {lightblue, style=dashed},
            {orange,    style=solid},
          }
        }
        \begin{figure}[t]
          \centering
          \input{figures/mnist/accuracy_k.tex}%
          \label{fig:mnist}
        \end{figure}
      \end{column}
    \end{columns}
  \end{tcolorbox}

  \begin{tcolorbox}
    \blocktitle{Recovery of sparse factors}

    \begin{columns}
      \begin{column}{0.5\textwidth}
        Motivated by compressive sensing, we generate sparse factors \( L
        \) to be recovered from measurements \( L L^{\top} \). C\( k \)-NN
        recovers \( L \) with near perfect accuracy over varying densities.
      \end{column}
      \begin{column}{0.5\textwidth}
        % \begin{figure}[h!]
        %   \centering
        %   \includegraphics[width=\textwidth]{data/s_score.png}
        % \end{figure}
        \pgfplotsset{
          cycle list={
            {silver,    style=densely dotted},
            {lightblue, style=dashed},
            {seagreen,  style=dashdotted},
            {orange,    style=solid},
          }
        }
        \begin{figure}[t]
          \centering
          \input{figures/recover/accuracy_s.tex}
          \label{fig:recover_acc}
        \end{figure}
      \end{column}
    \end{columns}
  \end{tcolorbox}

  \begin{tcolorbox}
    \blocktitle{Better KL divergence with sparser factors}

    \begin{columns}
      \begin{column}{0.5\textwidth}
        Plugging the selection algorithm into Cholesky
        factorization leads to better KL divergence for
        the same number of nonzero entries as \( k \)-NN.
      \end{column}
      \begin{column}{0.5\textwidth}
        % \begin{figure}[h!]
        %   \centering
        %   \includegraphics[width=\textwidth]{data/rho_kl-div.png}
        % \end{figure}
        \pgfplotsset{
          cycle list={
            {lightblue, style=dashed,         mark=*},
            {seagreen,  style=dashdotted,     mark=o},
            % {silver,    style=densely dotted, mark=triangle*},
            {orange,    style=solid,          mark=square*},
            {rust,      style=dotted,         mark=square},
          }
        }
        \begin{figure}[t]
          \centering
          \input{figures/cholesky/rho_kl_div.tex}
          \label{fig:chol_rho}
        \end{figure}
      \end{column}
    \end{columns}
  \end{tcolorbox}

  \begin{tcolorbox}
    \blocktitle{Preconditioning with conjugate gradient}

    \begin{columns}
      \begin{column}{0.5\textwidth}
        Because minimizing KL divergence minimizes the Kaporin condition
        number, our method needs fewer iterations of the conjugate
        gradient to solve linear systems \( \CM \vec{x} = \vec{y} \).
      \end{column}
      \begin{column}{0.5\textwidth}
        % \begin{figure}[h!]
        %   \centering
        %   \includegraphics[width=\textwidth]{data/n_iter-res.png}
        % \end{figure}
        \pgfplotsset{
          cycle list={
            {lightblue, style=dashed,         mark=*},
            {seagreen,  style=dashdotted,     mark=o},
            % {silver,    style=densely dotted, mark=triangle*},
            {orange,    style=solid,          mark=square*},
            {rust,      style=dotted,         mark=square},
          }
        }
        \begin{figure}[t]
          \centering
          \input{figures/cg/n_iter-res.tex}
          \label{fig:cg_iter}
        \end{figure}
      \end{column}
    \end{columns}
  \end{tcolorbox}

\end{column}

\end{columns}

% \printbibliography

\end{frame}
\end{document}
